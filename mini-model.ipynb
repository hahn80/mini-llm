{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import string\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path, sequence_length, min_word_freq=8):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.tokens, self.vocab = self.load_and_preprocess(\n",
    "            file_path, min_word_freq=min_word_freq\n",
    "        )\n",
    "        self.data = self.create_sequences()\n",
    "\n",
    "    def load_and_preprocess(self, file_path, min_word_freq=8):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read().lower()\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "        # Tokenize the text using spaces\n",
    "        tokens = text.split()\n",
    "\n",
    "        # Count word frequencies\n",
    "        word_freq = Counter(tokens)\n",
    "\n",
    "        # Build vocabulary with words that occur more than `min_word_freq`\n",
    "        vocab = {\"<UNK>\": 0}\n",
    "        for word, freq in word_freq.items():\n",
    "            if freq >= min_word_freq:\n",
    "                vocab[word] = len(vocab)\n",
    "\n",
    "        # Replace rare words with <UNK>\n",
    "        tokens = [\n",
    "            word if word_freq[word] >= min_word_freq else \"<UNK>\" for word in tokens\n",
    "        ]\n",
    "\n",
    "        return tokens, vocab\n",
    "\n",
    "    def create_sequences(self):\n",
    "        sequences = []\n",
    "        for i in range(0, len(self.tokens) - self.sequence_length):\n",
    "            seq = self.tokens[i : i + self.sequence_length]\n",
    "            seq_idx = [self.vocab.get(word, 0) for word in seq]\n",
    "            sequences.append(seq_idx)\n",
    "        return sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = torch.tensor(self.data[idx])\n",
    "        return sequence[:-1], sequence[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence:  tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 10, 10,  1,  2, 12, 13, 11,\n",
      "        14])\n",
      "Target Sequence:  tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 10, 10,  1,  2, 12, 13, 11, 14,\n",
      "        15])\n"
     ]
    }
   ],
   "source": [
    "file_path = \"input.txt\"\n",
    "sequence_length = 20\n",
    "dataset = TextDataset(file_path, sequence_length)\n",
    "\n",
    "for x, y in dataset:\n",
    "    print(\"Input Sequence: \", x)\n",
    "    print(\"Target Sequence: \", y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2233"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first',\n",
       " 'citizen',\n",
       " 'before',\n",
       " 'we',\n",
       " 'proceed',\n",
       " 'any',\n",
       " 'further',\n",
       " 'hear',\n",
       " 'me',\n",
       " 'speak',\n",
       " 'all',\n",
       " 'speak',\n",
       " 'speak',\n",
       " 'first',\n",
       " 'citizen',\n",
       " 'you',\n",
       " 'are',\n",
       " 'all',\n",
       " 'resolved',\n",
       " 'rather',\n",
       " 'to',\n",
       " 'die',\n",
       " 'than',\n",
       " 'to',\n",
       " '<UNK>',\n",
       " 'all',\n",
       " 'resolved',\n",
       " 'resolved',\n",
       " 'first',\n",
       " 'citizen']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tokens[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Einsum does matrix multiplication for query*keys for each training example\n",
    "        attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        # Add skip connection, run through normalization and finally dropout\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "\n",
    "        out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        # Handling the case when mask is None\n",
    "        if mask is None:\n",
    "            mask = torch.zeros((seq_length, seq_length), device=self.device).type(\n",
    "                torch.bool\n",
    "            )\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dataset.vocab\n",
    "vocab_size = len(vocab)  # Replace with your vocabulary size\n",
    "embed_size = 128  # Embedding size\n",
    "num_layers = 2  # Number of Transformer layers\n",
    "heads = 4  # Number of heads in multi-head attention\n",
    "forward_expansion = 4\n",
    "dropout = 0.2  # Dropout rate\n",
    "max_length = sequence_length  # Maximum length of a sequence\n",
    "idx_to_word = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 3\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = Transformer(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_size=embed_size,\n",
    "    num_layers=num_layers,\n",
    "    heads=heads,\n",
    "    device=device,\n",
    "    forward_expansion=forward_expansion,\n",
    "    dropout=dropout,\n",
    "    max_length=max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/3]: 100%|██████████| 6333/6333 [01:29<00:00, 70.65it/s, loss=4.77]\n",
      "Epoch [2/3]: 100%|██████████| 6333/6333 [01:27<00:00, 72.56it/s, loss=4.05]\n",
      "Epoch [3/3]: 100%|██████████| 6333/6333 [01:27<00:00, 72.60it/s, loss=3.81]\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.to(device)\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    # Wrap the data_loader with tqdm for a progress bar\n",
    "    progress_bar = tqdm(enumerate(data_loader), total=len(data_loader))\n",
    "    for batch_idx, (data, targets) in progress_bar:\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        scores = model(data, None)\n",
    "        scores = scores.view(-1, scores.size(-1))\n",
    "        targets = targets.view(-1)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        progress_bar.set_postfix(loss=total_loss / (batch_idx + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "model.to(device)\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model's state dictionary\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (word_embedding): Embedding(2233, 128)\n",
       "  (position_embedding): Embedding(20, 128)\n",
       "  (layers): ModuleList(\n",
       "    (0-1): 2 x TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (values): Linear(in_features=32, out_features=32, bias=False)\n",
       "        (keys): Linear(in_features=32, out_features=32, bias=False)\n",
       "        (queries): Linear(in_features=32, out_features=32, bias=False)\n",
       "        (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=128, out_features=2233, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'he', 'hath', 'done', 'famously', 'he']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"what he hath done famously he\"\n",
    "input_text = input_text.lower()\n",
    "input_text = input_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "words = input_text.split()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53, 93, 108, 44, 0, 93]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = dataset.vocab\n",
    "sequence = [vocab.get(word, vocab[\"<UNK>\"]) for word in words]\n",
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 53,  93, 108,  44,   0,  93]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = torch.tensor([sequence], dtype=torch.long)\n",
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = input_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable gradient calculations\n",
    "with torch.no_grad():\n",
    "    # Feed the tensor to the model\n",
    "    output = model(input_tensor, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3974e-02, 2.9498e-05, 3.6062e-06,  ..., 2.5875e-09, 1.9804e-08,\n",
       "        4.2472e-07], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the last word logits and apply softmax\n",
    "last_word_logits = output[0, -1, :]\n",
    "probabilities = F.softmax(last_word_logits, dim=0)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the most likely next word index\n",
    "next_word_idx = torch.argmax(probabilities).item()\n",
    "next_word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hath'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_word[next_word_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input_text):\n",
    "    input_text = input_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    words = input_text.lower().split()\n",
    "    sequence = [vocab.get(word, vocab[\"<UNK>\"]) for word in words]\n",
    "\n",
    "    for _ in range(10):\n",
    "        # Tokenize the current sequence of words and convert to a tensor\n",
    "        input_tensor = torch.tensor([sequence], dtype=torch.long)\n",
    "        input_tensor = input_tensor.to(device)\n",
    "\n",
    "        # Get the model's prediction for the next word\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor, None)\n",
    "\n",
    "        # Extract the last word (the next word prediction)\n",
    "        last_word_logits = output[0, -1, :]\n",
    "        probabilities = F.softmax(last_word_logits, dim=0)\n",
    "        next_word_idx = torch.argmax(probabilities).item()\n",
    "\n",
    "        # Find the predicted word and append to the current sequence\n",
    "        predicted_word = idx_to_word.get(next_word_idx)\n",
    "\n",
    "        words.append(predicted_word)\n",
    "        sequence.append(next_word_idx)\n",
    "\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"what he hath done famously he\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what he hath done famously he hath he hath he hath he hath he hath he'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
